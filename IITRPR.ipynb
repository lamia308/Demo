{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1DYHcNWWJbYjH1oGSien2PzHObQpIDHsY",
      "authorship_tag": "ABX9TyMyawVsZ5DGQrVx4LBFmeCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamia308/Demo/blob/master/IITRPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summer Internship at IIT Ropar**\n",
        "\n",
        "Author: Lamia Zakir\n",
        "\n",
        "Problem Definition : Extract features from given Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "n3kMAYqAFkav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWrmm3DJ6kvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eadd08b-06ee-4b0e-b02f-f970d454bcf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing file: /content/drive/MyDrive/Dataset/train_features_0.jsonl\n",
            "Expecting ',' delimiter: line 1 column 227 (char 226)\n",
            "Error processing file: /content/drive/MyDrive/Dataset/train_features_1.jsonl\n",
            "Unterminated string starting at: line 1 column 4578 (char 4577)\n",
            "Error processing file: /content/drive/MyDrive/Dataset/train_features_2.jsonl\n",
            "Expecting ',' delimiter: line 1 column 2525 (char 2524)\n",
            "Error processing file: /content/drive/MyDrive/Dataset/train_features_3.jsonl\n",
            "Expecting ',' delimiter: line 1 column 360 (char 359)\n",
            "Error processing file: /content/drive/MyDrive/Dataset/train_features_4.jsonl\n",
            "Expecting value: line 1 column 640 (char 639)\n",
            "Error processing file: /content/drive/MyDrive/Dataset/train_features_5.jsonl\n",
            "Expecting property name enclosed in double quotes: line 1 column 1677 (char 1676)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return [json.loads(line) for line in lines]\n",
        "\n",
        "# List of file paths\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Dataset/train_features_0.jsonl',\n",
        "    '/content/drive/MyDrive/Dataset/train_features_1.jsonl',\n",
        "    '/content/drive/MyDrive/Dataset/train_features_2.jsonl',\n",
        "    '/content/drive/MyDrive/Dataset/train_features_3.jsonl',\n",
        "    '/content/drive/MyDrive/Dataset/train_features_4.jsonl',\n",
        "    '/content/drive/MyDrive/Dataset/train_features_5.jsonl'\n",
        "]\n",
        "\n",
        "# List to store nested dictionaries\n",
        "nested_dicts = []\n",
        "\n",
        "# Read and process each file\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        data = read_jsonl_file(file_path)\n",
        "\n",
        "        for item in data:\n",
        "            nested_dict = {}\n",
        "\n",
        "            nested_dict['sha256'] = item.get('sha256')\n",
        "            nested_dict['md5'] = item.get('md5')\n",
        "            nested_dict['appeared'] = item.get('appeared')\n",
        "            nested_dict['label'] = item.get('label')\n",
        "            nested_dict['avclass'] = item.get('avclass')\n",
        "            nested_dict['histogram'] = item.get('histogram')\n",
        "            nested_dict['byteentropy'] = item.get('byteentropy')\n",
        "            nested_dict['strings'] = item.get('strings')\n",
        "            nested_dict['general'] = item.get('general')\n",
        "            nested_dict['header'] = item.get('header')\n",
        "            nested_dict['section'] = item.get('section')\n",
        "            nested_dict['imports'] = item.get('imports')\n",
        "\n",
        "            nested_dicts.append(nested_dict)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error processing file: {file_path}\")\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "# Print the nested dictionaries\n",
        "for index, nested_dict in enumerate(nested_dicts):\n",
        "    print(f\"Nested Dictionary {index + 1}:\")\n",
        "    print(nested_dict)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                try:\n",
        "                    entry = json.loads(line)\n",
        "                    data.append(entry)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"JSONDecodeError: {e}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/train_features_0.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCM3EX0EIJ8R",
        "outputId": "b2554010-e424-4374-f8d2-89689083133e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSONDecodeError: Expecting ',' delimiter: line 1 column 227 (char 226)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if field_name == 'general' or field_name == 'header' or field_name == 'section' or field_name == 'strings':\n",
        "                    if isinstance(value, dict):\n",
        "                        for subfield in value:\n",
        "                            row.append(value[subfield])\n",
        "                elif isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                    row.append(value)\n",
        "                else:\n",
        "                    row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n"
      ],
      "metadata": {
        "id": "UOTavdgWcqpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/train_features_0.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheet0.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MGUkgr1N5-w",
        "outputId": "fbd06baf-0b3a-4593-c0aa-e60a6ff7eecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n",
            "Error: Invalid line in JSONL file: line contains invalid json: Expecting ',' delimiter: line 1 column 227 (char 226) (line 696)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonlines\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"InvalidLineError: {e}\")\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/train_features_1.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvFY_hVdILEZ",
        "outputId": "8cad950d-212f-45e8-b25f-a898a239119b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidLineError: line contains invalid json: Unterminated string starting at: line 1 column 4578 (char 4577) (line 715)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/train_features_1.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheet1.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBuEYg4yIVzg",
        "outputId": "e6a516ec-4eb3-4587-d22f-cd7e5090967d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Invalid line in JSONL file: line contains invalid json: Unterminated string starting at: line 1 column 4578 (char 4577) (line 715)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonlines\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"InvalidLineError: {e}\")\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/train_features_2.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhwSuGb5TgCY",
        "outputId": "171dd422-4f80-4573-c626-201362ade0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidLineError: line contains invalid json: Expecting ',' delimiter: line 1 column 2525 (char 2524) (line 228)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/train_features_2.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheet2.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSinA9HGUKxO",
        "outputId": "6e9c9eed-bab7-4285-a4b2-7f025437f9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Invalid line in JSONL file: line contains invalid json: Expecting ',' delimiter: line 1 column 2525 (char 2524) (line 228)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonlines\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"InvalidLineError: {e}\")\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/train_features_3.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLj0NIt9VeYi",
        "outputId": "93803ea8-3443-45bc-acd0-d55efa7ed4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidLineError: line contains invalid json: Expecting ',' delimiter: line 1 column 360 (char 359) (line 90)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/train_features_3.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheet3.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a01NWioVxMO",
        "outputId": "ee702b7d-3f4d-40b3-d162-c0086e2e2f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Error: Invalid line in JSONL file: line contains invalid json: Expecting ',' delimiter: line 1 column 360 (char 359) (line 90)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonlines\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"InvalidLineError: {e}\")\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/train_features_4.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv87rYhCY0jt",
        "outputId": "aceec6c2-809d-48dc-c8f6-5a7682c3d88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidLineError: line contains invalid json: Expecting value: line 1 column 640 (char 639) (line 854)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/train_features_4.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheet4.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RII4PJJqY7kx",
        "outputId": "66be3cf4-7776-4c14-c584-75e3320517c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Error: Invalid line in JSONL file: line contains invalid json: Expecting value: line 1 column 640 (char 639) (line 854)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonlines\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"InvalidLineError: {e}\")\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/train_features_5.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJfLIHv6ZKSo",
        "outputId": "42342136-559e-4b50-db03-17b50b8d9796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidLineError: line contains invalid json: Expecting property name enclosed in double quotes: line 1 column 1677 (char 1676) (line 839)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/train_features_5.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheet5.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_N-NSvXZTxM",
        "outputId": "ca6da6d9-ab33-4840-9e22-ccdd21338493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Error: Invalid line in JSONL file: line contains invalid json: Expecting property name enclosed in double quotes: line 1 column 1677 (char 1676) (line 839)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import jsonlines\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"InvalidLineError: {e}\")\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/drive/MyDrive/Dataset/test_features.jsonl'\n",
        "jsonl_data = read_jsonl_file(file_path)\n",
        "\n",
        "# Extracting field names\n",
        "field_names = set()\n",
        "for entry in jsonl_data:\n",
        "    field_names.update(entry.keys())\n",
        "\n",
        "# Printing field names\n",
        "for field_name in field_names:\n",
        "    print(field_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vezCbz0yapb9",
        "outputId": "5f446231-9ef6-4a2e-ef15-e8e1ec657ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidLineError: line contains invalid json: Expecting ',' delimiter: line 1 column 1955 (char 1954) (line 1016)\n",
            "general\n",
            "label\n",
            "byteentropy\n",
            "section\n",
            "datadirectories\n",
            "avclass\n",
            "header\n",
            "imports\n",
            "md5\n",
            "exports\n",
            "histogram\n",
            "sha256\n",
            "appeared\n",
            "strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def read_jsonl_file(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with jsonlines.open(file_path) as reader:\n",
        "            for line in reader:\n",
        "                data.append(line)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "    except jsonlines.InvalidLineError as e:\n",
        "        print(f\"Error: Invalid line in JSONL file: {str(e)}\")\n",
        "    return data\n",
        "\n",
        "def extract_data(jsonl_data, field_names):\n",
        "    extracted_data = []\n",
        "    for entry in jsonl_data:\n",
        "        row = []\n",
        "        for field_name in field_names:\n",
        "            if field_name in entry:\n",
        "                value = entry[field_name]\n",
        "                if isinstance(value, list):\n",
        "                    value = ', '.join([str(item) for item in value])\n",
        "                elif isinstance(value, dict):\n",
        "                    value = str(value)\n",
        "                row.append(value)\n",
        "            else:\n",
        "                row.append('')\n",
        "        extracted_data.append(row)\n",
        "    return extracted_data\n",
        "\n",
        "def save_as_csv(file_path, data, field_names):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(field_names)\n",
        "        writer.writerows(data)\n",
        "    print('CSV file saved successfully.')\n",
        "\n",
        "# Example usage\n",
        "file_path ='/content/drive/MyDrive/Dataset/test_features.jsonl'\n",
        "csv_file_path = '/content/drive/MyDrive/Output/sheettest.csv'\n",
        "\n",
        "field_names = ['datadirectories', 'md5', 'appeared', 'histogram', 'imports', 'general', 'byteentropy',\n",
        "               'strings', 'avclass', 'header', 'label', 'section', 'sha256', 'exports']\n",
        "\n",
        "if os.path.isfile(file_path):\n",
        "    jsonl_data = read_jsonl_file(file_path)\n",
        "    if jsonl_data:\n",
        "\n",
        "        extracted_data = extract_data(jsonl_data, field_names)\n",
        "        save_as_csv(csv_file_path, extracted_data, field_names)\n",
        "    else:\n",
        "        print(f\"Error: No valid data found in '{file_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqInjeS1awwA",
        "outputId": "3a2c447b-f96f-4b0a-8c19-d582fbd16f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Error: Invalid line in JSONL file: line contains invalid json: Expecting ',' delimiter: line 1 column 1955 (char 1954) (line 1016)\n",
            "CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_files = ['/content/drive/MyDrive/Output/sheet0.csv',\n",
        "               '/content/drive/MyDrive/Output/sheet1.csv',\n",
        "               '/content/drive/MyDrive/Output/sheet2.csv',\n",
        "               '/content/drive/MyDrive/Output/sheet3.csv',\n",
        "               '/content/drive/MyDrive/Output/sheet4.csv',\n",
        "               '/content/drive/MyDrive/Output/sheet5.csv']\n",
        "\n",
        "selected_features = ['label', 'appeared', 'exports', 'imports', 'byteentropy', 'header', 'general']\n",
        "\n",
        "train_data = pd.DataFrame()\n",
        "for file_path in train_files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df_selected = df[selected_features]\n",
        "    df_selected.dropna(inplace=True)\n",
        "    train_data = train_data.append(df_selected, ignore_index=True)\n",
        "\n",
        "train_data.head(1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "bG-PBpPCiuhj",
        "outputId": "d8890beb-46ae-4308-c088-a72a9eb1584a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-b2d315ed1104>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_selected.dropna(inplace=True)\n",
            "<ipython-input-32-b2d315ed1104>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df_selected, ignore_index=True)\n",
            "<ipython-input-32-b2d315ed1104>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_selected.dropna(inplace=True)\n",
            "<ipython-input-32-b2d315ed1104>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df_selected, ignore_index=True)\n",
            "<ipython-input-32-b2d315ed1104>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_selected.dropna(inplace=True)\n",
            "<ipython-input-32-b2d315ed1104>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df_selected, ignore_index=True)\n",
            "<ipython-input-32-b2d315ed1104>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_selected.dropna(inplace=True)\n",
            "<ipython-input-32-b2d315ed1104>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df_selected, ignore_index=True)\n",
            "<ipython-input-32-b2d315ed1104>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_selected.dropna(inplace=True)\n",
            "<ipython-input-32-b2d315ed1104>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df_selected, ignore_index=True)\n",
            "<ipython-input-32-b2d315ed1104>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_selected.dropna(inplace=True)\n",
            "<ipython-input-32-b2d315ed1104>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_data = train_data.append(df_selected, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label appeared                             exports  \\\n",
              "0      0  2007-04  _MainWndProc@16, _StubFileWrite@12   \n",
              "\n",
              "                                             imports  \\\n",
              "0  {'KERNEL32.dll': ['lstrcpyA', 'GetCommandLineA...   \n",
              "\n",
              "                                         byteentropy  \\\n",
              "0  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
              "\n",
              "                                              header  \\\n",
              "0  {'coff': {'timestamp': 956673432, 'machine': '...   \n",
              "\n",
              "                                             general  \n",
              "0  {'size': 1643293, 'vsize': 28672, 'has_debug':...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d2ab6f5-7802-4375-ac56-917ee01d6c23\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>appeared</th>\n",
              "      <th>exports</th>\n",
              "      <th>imports</th>\n",
              "      <th>byteentropy</th>\n",
              "      <th>header</th>\n",
              "      <th>general</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2007-04</td>\n",
              "      <td>_MainWndProc@16, _StubFileWrite@12</td>\n",
              "      <td>{'KERNEL32.dll': ['lstrcpyA', 'GetCommandLineA...</td>\n",
              "      <td>0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>{'coff': {'timestamp': 956673432, 'machine': '...</td>\n",
              "      <td>{'size': 1643293, 'vsize': 28672, 'has_debug':...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d2ab6f5-7802-4375-ac56-917ee01d6c23')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d2ab6f5-7802-4375-ac56-917ee01d6c23 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d2ab6f5-7802-4375-ac56-917ee01d6c23');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mbeo-ASnSFI",
        "outputId": "01d80897-6d84-498e-9718-66ca4609647c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['label', 'appeared', 'exports', 'imports', 'byteentropy', 'header',\n",
              "       'general'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe(include = 'all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "YYl2LjL_lNjO",
        "outputId": "94e0a047-3292-4794-8238-24429ae8e469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             label appeared  \\\n",
              "count   560.000000      560   \n",
              "unique         NaN       20   \n",
              "top            NaN  2018-07   \n",
              "freq           NaN      131   \n",
              "mean     -0.058929      NaN   \n",
              "std       0.530054      NaN   \n",
              "min      -1.000000      NaN   \n",
              "25%       0.000000      NaN   \n",
              "50%       0.000000      NaN   \n",
              "75%       0.000000      NaN   \n",
              "max       1.000000      NaN   \n",
              "\n",
              "                                                  exports imports  \\\n",
              "count                                                 560     560   \n",
              "unique                                                426     511   \n",
              "top     DllCanUnloadNow, DllGetClassObject, DllRegiste...      {}   \n",
              "freq                                                   49      17   \n",
              "mean                                                  NaN     NaN   \n",
              "std                                                   NaN     NaN   \n",
              "min                                                   NaN     NaN   \n",
              "25%                                                   NaN     NaN   \n",
              "50%                                                   NaN     NaN   \n",
              "75%                                                   NaN     NaN   \n",
              "max                                                   NaN     NaN   \n",
              "\n",
              "                                              byteentropy  \\\n",
              "count                                                 560   \n",
              "unique                                                560   \n",
              "top     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
              "freq                                                    1   \n",
              "mean                                                  NaN   \n",
              "std                                                   NaN   \n",
              "min                                                   NaN   \n",
              "25%                                                   NaN   \n",
              "50%                                                   NaN   \n",
              "75%                                                   NaN   \n",
              "max                                                   NaN   \n",
              "\n",
              "                                                   header  \\\n",
              "count                                                 560   \n",
              "unique                                                536   \n",
              "top     {'coff': {'timestamp': 1494505297, 'machine': ...   \n",
              "freq                                                    7   \n",
              "mean                                                  NaN   \n",
              "std                                                   NaN   \n",
              "min                                                   NaN   \n",
              "25%                                                   NaN   \n",
              "50%                                                   NaN   \n",
              "75%                                                   NaN   \n",
              "max                                                   NaN   \n",
              "\n",
              "                                                  general  \n",
              "count                                                 560  \n",
              "unique                                                536  \n",
              "top     {'size': 1267712, 'vsize': 1290240, 'has_debug...  \n",
              "freq                                                    7  \n",
              "mean                                                  NaN  \n",
              "std                                                   NaN  \n",
              "min                                                   NaN  \n",
              "25%                                                   NaN  \n",
              "50%                                                   NaN  \n",
              "75%                                                   NaN  \n",
              "max                                                   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-268d8cf4-b0ba-48fe-9834-9b060fc5ef44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>appeared</th>\n",
              "      <th>exports</th>\n",
              "      <th>imports</th>\n",
              "      <th>byteentropy</th>\n",
              "      <th>header</th>\n",
              "      <th>general</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>560.000000</td>\n",
              "      <td>560</td>\n",
              "      <td>560</td>\n",
              "      <td>560</td>\n",
              "      <td>560</td>\n",
              "      <td>560</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>20</td>\n",
              "      <td>426</td>\n",
              "      <td>511</td>\n",
              "      <td>560</td>\n",
              "      <td>536</td>\n",
              "      <td>536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018-07</td>\n",
              "      <td>DllCanUnloadNow, DllGetClassObject, DllRegiste...</td>\n",
              "      <td>{}</td>\n",
              "      <td>0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>{'coff': {'timestamp': 1494505297, 'machine': ...</td>\n",
              "      <td>{'size': 1267712, 'vsize': 1290240, 'has_debug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>131</td>\n",
              "      <td>49</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.058929</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.530054</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-268d8cf4-b0ba-48fe-9834-9b060fc5ef44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-268d8cf4-b0ba-48fe-9834-9b060fc5ef44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-268d8cf4-b0ba-48fe-9834-9b060fc5ef44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Rows where lebel == -1"
      ],
      "metadata": {
        "id": "RKY6UrC0nwEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data[train_data['label'] != -1]"
      ],
      "metadata": {
        "id": "OCyCMxYqnc5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe(include = 'all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "TFw-Bt-gn4Rt",
        "outputId": "2a4b23dc-803a-4504-fae1-5bb45f732716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             label appeared  \\\n",
              "count   464.000000      464   \n",
              "unique         NaN       20   \n",
              "top            NaN  2018-07   \n",
              "freq           NaN       94   \n",
              "mean      0.135776      NaN   \n",
              "std       0.342920      NaN   \n",
              "min       0.000000      NaN   \n",
              "25%       0.000000      NaN   \n",
              "50%       0.000000      NaN   \n",
              "75%       0.000000      NaN   \n",
              "max       1.000000      NaN   \n",
              "\n",
              "                                                  exports imports  \\\n",
              "count                                                 464     464   \n",
              "unique                                                354     423   \n",
              "top     DllCanUnloadNow, DllGetClassObject, DllRegiste...      {}   \n",
              "freq                                                   44      15   \n",
              "mean                                                  NaN     NaN   \n",
              "std                                                   NaN     NaN   \n",
              "min                                                   NaN     NaN   \n",
              "25%                                                   NaN     NaN   \n",
              "50%                                                   NaN     NaN   \n",
              "75%                                                   NaN     NaN   \n",
              "max                                                   NaN     NaN   \n",
              "\n",
              "                                              byteentropy  \\\n",
              "count                                                 464   \n",
              "unique                                                464   \n",
              "top     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
              "freq                                                    1   \n",
              "mean                                                  NaN   \n",
              "std                                                   NaN   \n",
              "min                                                   NaN   \n",
              "25%                                                   NaN   \n",
              "50%                                                   NaN   \n",
              "75%                                                   NaN   \n",
              "max                                                   NaN   \n",
              "\n",
              "                                                   header  \\\n",
              "count                                                 464   \n",
              "unique                                                444   \n",
              "top     {'coff': {'timestamp': 1494505297, 'machine': ...   \n",
              "freq                                                    7   \n",
              "mean                                                  NaN   \n",
              "std                                                   NaN   \n",
              "min                                                   NaN   \n",
              "25%                                                   NaN   \n",
              "50%                                                   NaN   \n",
              "75%                                                   NaN   \n",
              "max                                                   NaN   \n",
              "\n",
              "                                                  general  \n",
              "count                                                 464  \n",
              "unique                                                446  \n",
              "top     {'size': 5267459, 'vsize': 5267456, 'has_debug...  \n",
              "freq                                                    7  \n",
              "mean                                                  NaN  \n",
              "std                                                   NaN  \n",
              "min                                                   NaN  \n",
              "25%                                                   NaN  \n",
              "50%                                                   NaN  \n",
              "75%                                                   NaN  \n",
              "max                                                   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aad821af-6192-449d-88f0-d7ab5a7b8c46\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>appeared</th>\n",
              "      <th>exports</th>\n",
              "      <th>imports</th>\n",
              "      <th>byteentropy</th>\n",
              "      <th>header</th>\n",
              "      <th>general</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>464.000000</td>\n",
              "      <td>464</td>\n",
              "      <td>464</td>\n",
              "      <td>464</td>\n",
              "      <td>464</td>\n",
              "      <td>464</td>\n",
              "      <td>464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>20</td>\n",
              "      <td>354</td>\n",
              "      <td>423</td>\n",
              "      <td>464</td>\n",
              "      <td>444</td>\n",
              "      <td>446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2018-07</td>\n",
              "      <td>DllCanUnloadNow, DllGetClassObject, DllRegiste...</td>\n",
              "      <td>{}</td>\n",
              "      <td>0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>{'coff': {'timestamp': 1494505297, 'machine': ...</td>\n",
              "      <td>{'size': 5267459, 'vsize': 5267456, 'has_debug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>94</td>\n",
              "      <td>44</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.135776</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.342920</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aad821af-6192-449d-88f0-d7ab5a7b8c46')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aad821af-6192-449d-88f0-d7ab5a7b8c46 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aad821af-6192-449d-88f0-d7ab5a7b8c46');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Dataset/train_features_0.jsonl'\n",
        "csv_file = '/content/drive/MyDrive/Output/general0.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "_ClO1zU2ayJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Dataset/train_features_1.jsonl'\n",
        "csv_file = '/content/drive/MyDrive/Output/general1.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "Q5ubkNKYpdox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Dataset/train_features_2.jsonl'\n",
        "csv_file = '/content/drive/MyDrive/Output/general2.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "kSkn5bvlpjQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Dataset/train_features_3.jsonl'\n",
        "csv_file = '/content/drive/MyDrive/Output/general3.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "C-8iFdpvpoTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Dataset/train_features_4.jsonl'\n",
        "csv_file = '/content/drive/MyDrive/Output/general4.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "wbKTTCjXptRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Dataset/train_features_5.jsonl'\n",
        "csv_file = '/content/drive/MyDrive/Output/general5.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "2Cf6GAwcpz80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "jsonl_file = '/content/drive/MyDrive/Output/sheettest.csv'\n",
        "csv_file = '/content/drive/MyDrive/Output/generaltest.csv'\n",
        "\n",
        "# Extract key-value pairs from 'general' feature in JSONL file\n",
        "data = []\n",
        "with open(jsonl_file, 'r') as f:\n",
        "  try:\n",
        "    for line in f:\n",
        "        json_data = json.loads(line)\n",
        "        general = json_data.get('general', {})\n",
        "        data.append(general)\n",
        "  except:\n",
        "    pass\n",
        "  finally:\n",
        "# Extract unique keys from all 'general' dictionaries\n",
        "    keys = set()\n",
        "    for item in data:\n",
        "        keys.update(item.keys())\n",
        "\n",
        "    # Sort the keys alphabetically\n",
        "    sorted_keys = sorted(keys)\n",
        "\n",
        "    # Write the data to CSV file\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted_keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "ogeyyVs9a4y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet0.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/header0.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Vqx0URxDQvMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet1.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/header1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "bIgZrpPZRdVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet2.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/header2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "nbTw1E41Rr1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet3.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/header3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "lJwme9yYR6FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet4.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/header4.csv', index=False)\n"
      ],
      "metadata": {
        "id": "9cKDzPBxSABC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet5.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/header5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rzHDETQNSwOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheettest.csv')\n",
        "\n",
        "# Extract data from the \"header\" column\n",
        "df['header'] = df['header'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['header'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/headertest.csv', index=False)\n"
      ],
      "metadata": {
        "id": "yQFkAbtiS1PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet0.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/section0.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NGVPd9qEUZae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet1.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/section1.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dfojQpK3W9oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet2.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/section2.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XMdVegipXGoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet3.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/section3.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oCuaaEIqXRuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet4.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/section4.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oM3rdIupXihs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet5.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/section5.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lp3Hqcg8XpSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheettest.csv')\n",
        "\n",
        "# Extract data from the \"section\" column\n",
        "df['section'] = df['section'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['section'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/sectiontest.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DfHCmyLRXt81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet0.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/strings0.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mzONHekpYdw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet1.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/strings1.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k0YxqH1cZHZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet2.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/strings2.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJSy47lXZSwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet3.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/strings3.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v_HI54nDZX9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet4.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/strings4.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GEW53GW6ZhLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheet5.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/strings5.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_raoFh4TZmIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/sheettest.csv')\n",
        "\n",
        "# Extract data from the \"strings\" column\n",
        "df['strings'] = df['strings'].apply(ast.literal_eval)\n",
        "\n",
        "# Define a function to flatten the nested dictionary\n",
        "def flatten_dict(data, prefix='', flattened_data=None):\n",
        "    if flattened_data is None:\n",
        "        flattened_data = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            flatten_dict(value, prefix=prefix + key + '_', flattened_data=flattened_data)\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flatten_dict(item, prefix=prefix + key + f'_{i}_', flattened_data=flattened_data)\n",
        "                else:\n",
        "                    flattened_data[prefix + key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened_data[prefix + key] = value\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "# Apply flattening to all rows\n",
        "flattened_data = df['strings'].apply(flatten_dict)\n",
        "\n",
        "# Create a DataFrame from flattened data\n",
        "df_flattened = pd.DataFrame(flattened_data.tolist())\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df_flattened.to_csv('/content/drive/MyDrive/Output/stringstest.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AsGe3uwnZrPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the final csvs\n"
      ],
      "metadata": {
        "id": "rpy48y4XbdEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheet0.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['appeared', 'label',]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet0.csv', index=False)\n"
      ],
      "metadata": {
        "id": "QK_gXM1k1wvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/general0.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['exports','has_debug',\t'has_relocations'\t,'has_resources',\t'has_signature',\t'has_tls',\t'imports',\t'size',\t'symbols',\t'vsize']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet0.csv', index=False)\n"
      ],
      "metadata": {
        "id": "FLPsK4e54k4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/header0.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet0.csv', index=False)\n"
      ],
      "metadata": {
        "id": "BKYKyyNM_SPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/strings0.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet0.csv', index=False)\n"
      ],
      "metadata": {
        "id": "uDUoRA7frxTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheet1.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = [ 'appeared','label']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "3VVWMYI553I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/general1.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['exports',\t'has_debug',\t'has_relocations',\t'has_resources',\t'has_signature',\t'has_tls',\t'imports',\t'size',\t'symbols',\t'vsize'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "HRE0Fm2d9END"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/header1.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Oo1U7Esz-6L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/strings1.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "eUTRCX3g_8oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheet2.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = [ 'appeared','label']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "DKW8bHqPAs5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/general2.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['exports',\t'has_debug',\t'has_relocations',\t'has_resources',\t'has_signature',\t'has_tls',\t'imports',\t'size',\t'symbols',\t'vsize'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "er_tvYNQA7AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/header2.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Gun3jru_Bh4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/strings2.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "jVvpm9JmCZJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheet3.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = [ 'appeared','label']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "yUj7lir3C30e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/general3.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['exports',\t'has_debug',\t'has_relocations',\t'has_resources',\t'has_signature',\t'has_tls',\t'imports',\t'size',\t'symbols',\t'vsize'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "n2WG0TRgELpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/header3.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "RxGVAf_nETh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/strings3.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "cuVFDnOVEj3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheet4.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = [ 'appeared','label']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet4.csv', index=False)\n"
      ],
      "metadata": {
        "id": "2aQ5YKK2EvzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/general4.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['exports',\t'has_debug',\t'has_relocations',\t'has_resources',\t'has_signature',\t'has_tls',\t'imports',\t'size',\t'symbols',\t'vsize'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet4.csv', index=False)\n"
      ],
      "metadata": {
        "id": "wsimFAWqFWnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/header4.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet4.csv', index=False)\n"
      ],
      "metadata": {
        "id": "g9FLUNRLFdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/strings4.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet4.csv', index=False)\n"
      ],
      "metadata": {
        "id": "bJ5cnq7mF94x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheet5.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = [ 'appeared','label']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "pI5cBaIFGKC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/general5.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['exports',\t'has_debug',\t'has_relocations',\t'has_resources',\t'has_signature',\t'has_tls',\t'imports',\t'size',\t'symbols',\t'vsize'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "RiVdMXoUG6lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/header5.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "NKWPhK11HCdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/strings5.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheet5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "vNaSGFKdHbor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/sheettest.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = [ 'appeared','label']\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheettest.csv', index=False)\n"
      ],
      "metadata": {
        "id": "HmTQdrDvHhze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/headertest.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['coff_timestamp','coff_machine','optional_subsystem','optional_magic','optional_sizeof_code'\t,'optional_sizeof_headers'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheettest.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Ko47C4PBJ9eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in a directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/stringstest.csv')\n",
        "\n",
        "# Create an empty DataFrame to store the combined columns\n",
        "#combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the current CSV file\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract the columns from the current file\n",
        "    selected_columns = ['numstrings',\t'avlength',\t'printables',\t'entropy',\t'paths',\t'urls',\t'registry',\t'MZ'\t]\n",
        "    extracted_columns = df[selected_columns]\n",
        "\n",
        "    # Append the extracted columns to the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, extracted_columns], axis=1)\n",
        "\n",
        "# Export the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Output/finalsheettest.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ot2pwfiaKnwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheet0.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_file0.csv', index=False)\n"
      ],
      "metadata": {
        "id": "mA3UCP7dC6KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheet1.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_file1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "PiFwXiN1GJG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheet2.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_file2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "pNTA_ifqGbJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheet3.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_file3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "6tc2SVJ1Gqy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheet4.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_file4.csv', index=False)\n"
      ],
      "metadata": {
        "id": "i6DdimAlIHef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheet5.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_file5.csv', index=False)\n"
      ],
      "metadata": {
        "id": "VJTf5iFvIWpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/finalsheettest.csv')\n",
        "\n",
        "# Create dictionaries to map the string values to numerical values for both columns\n",
        "subsystem_mapping = {\n",
        "    'WINDOWS_GUI': 0,\n",
        "    'NATIVE': 1\n",
        "}\n",
        "\n",
        "coff_machine_mapping = {\n",
        "    'I386': 0,\n",
        "    'AMD64': 1,\n",
        "    'ARM': 3\n",
        "}\n",
        "optional_magic_mapping = {\n",
        "    'PE32': 0,\n",
        "    'PE32_PLUS': 1\n",
        "}\n",
        "\n",
        "# Use the map() function to replace the string values with numerical values for both columns\n",
        "df['optional_subsystem'] = df['optional_subsystem'].map(subsystem_mapping)\n",
        "df['coff_machine'] = df['coff_machine'].map(coff_machine_mapping)\n",
        "df['optional_magic'] = df['optional_magic'].map(optional_magic_mapping)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/Output/Aoutput_filetest.csv', index=False)\n"
      ],
      "metadata": {
        "id": "xxhhdVXXIoVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file0.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD8JRF8_GxOL",
        "outputId": "8a16bb08-8806-4533-c2b4-54619639757a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['appeared', 'label', 'exports', 'has_debug', 'has_relocations',\n",
            "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
            "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
            "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
            "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
            "       'entropy', 'paths', 'urls', 'registry', 'MZ'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column names to normalize\n",
        "columns_to_normalize = ['exports', 'has_debug', 'has_relocations',\n",
        "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "       'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file0.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the specified columns\n",
        "for column in columns_to_normalize:\n",
        "    # Find the minimum and maximum values of the column\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "\n",
        "    # Normalize the values using the formula (x - min_val) / (max_val - min_val)\n",
        "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Save the updated dataframe back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "RqlDunrwGKVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column names to normalize\n",
        "columns_to_normalize = ['exports', 'has_debug', 'has_relocations',\n",
        "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "       'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the specified columns\n",
        "for column in columns_to_normalize:\n",
        "    # Find the minimum and maximum values of the column\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "\n",
        "    # Normalize the values using the formula (x - min_val) / (max_val - min_val)\n",
        "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Save the updated dataframe back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "Vk7Ql9WVGn6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column names to normalize\n",
        "columns_to_normalize = ['exports', 'has_debug', 'has_relocations',\n",
        "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "       'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file2.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the specified columns\n",
        "for column in columns_to_normalize:\n",
        "    # Find the minimum and maximum values of the column\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "\n",
        "    # Normalize the values using the formula (x - min_val) / (max_val - min_val)\n",
        "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Save the updated dataframe back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "H5YuQBEQGwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column names to normalize\n",
        "columns_to_normalize = ['exports', 'has_debug', 'has_relocations',\n",
        "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "       'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the specified columns\n",
        "for column in columns_to_normalize:\n",
        "    # Find the minimum and maximum values of the column\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "\n",
        "    # Normalize the values using the formula (x - min_val) / (max_val - min_val)\n",
        "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Save the updated dataframe back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "cwcRGp2_G4fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column names to normalize\n",
        "columns_to_normalize = ['exports', 'has_debug', 'has_relocations',\n",
        "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "       'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file4.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the specified columns\n",
        "for column in columns_to_normalize:\n",
        "    # Find the minimum and maximum values of the column\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "\n",
        "    # Normalize the values using the formula (x - min_val) / (max_val - min_val)\n",
        "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Save the updated dataframe back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "24m1nkiJG9eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the column names to normalize\n",
        "columns_to_normalize = ['exports', 'has_debug', 'has_relocations',\n",
        "       'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "       'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "       'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "       'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "       'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/Output/Aoutput_file5.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the specified columns\n",
        "for column in columns_to_normalize:\n",
        "    # Find the minimum and maximum values of the column\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "\n",
        "    # Normalize the values using the formula (x - min_val) / (max_val - min_val)\n",
        "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Save the updated dataframe back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "RLQrH_veHE6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Get a list of all CSV files in the specified directory\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/Aoutput_file0.csv')\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/Aoutput_file1.csv')\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/Aoutput_file2.csv')\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/Aoutput_file3.csv')\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/Aoutput_file4.csv')\n",
        "csv_files = glob.glob('/content/drive/MyDrive/Output/Aoutput_file5.csv')\n",
        "\n",
        "# Iterate over each CSV file\n",
        "for file_path in csv_files:\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Filter out rows with a value of -1 in the \"label\" column\n",
        "    df_filtered = df[df['label'] != -1]\n",
        "\n",
        "    # Save the filtered DataFrame back to the same CSV file\n",
        "    df_filtered.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "ktH7310PLsMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/Aoutput_file0.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/Aoutput_file1.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/Aoutput_file2.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/Aoutput_file3.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/Aoutput_file4.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Output/Aoutput_file5.csv')\n",
        "\n",
        "# Get the input features (all columns except \"appeared\")\n",
        "features = df.drop('appeared', axis=1)\n",
        "\n",
        "# Get the target variable (\"appeared\" column)\n",
        "target = df['appeared']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SoyOhpHaS-Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths\n",
        "file_paths = ['/content/drive/MyDrive/Output/Aoutput_file0.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file1.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file2.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file3.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file4.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file5.csv']\n",
        "\n",
        "# Define the column names\n",
        "columns = ['appeared', 'label', 'exports', 'has_debug', 'has_relocations',\n",
        "           'has_resources', 'has_signature', 'has_tls', 'imports', 'size',\n",
        "           'symbols', 'vsize', 'coff_timestamp', 'coff_machine',\n",
        "           'optional_subsystem', 'optional_magic', 'optional_sizeof_code',\n",
        "           'optional_sizeof_headers', 'numstrings', 'avlength', 'printables',\n",
        "           'entropy', 'paths', 'urls', 'registry', 'MZ']\n",
        "\n",
        "# Read and concatenate the data from multiple files\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "combined_df = pd.concat(dfs)\n",
        "\n",
        "# Remove rows with missing values in specified columns\n",
        "combined_df.dropna(subset=columns, inplace=True)\n",
        "\n",
        "# Verify the missing values have been removed\n",
        "print(combined_df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYMm4RIAnqvs",
        "outputId": "03a9c8f0-5de8-48a1-b83e-7dc67c1ec1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "appeared                   0\n",
            "label                      0\n",
            "exports                    0\n",
            "has_debug                  0\n",
            "has_relocations            0\n",
            "has_resources              0\n",
            "has_signature              0\n",
            "has_tls                    0\n",
            "imports                    0\n",
            "size                       0\n",
            "symbols                    0\n",
            "vsize                      0\n",
            "coff_timestamp             0\n",
            "coff_machine               0\n",
            "optional_subsystem         0\n",
            "optional_magic             0\n",
            "optional_sizeof_code       0\n",
            "optional_sizeof_headers    0\n",
            "numstrings                 0\n",
            "avlength                   0\n",
            "printables                 0\n",
            "entropy                    0\n",
            "paths                      0\n",
            "urls                       0\n",
            "registry                   0\n",
            "MZ                         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "# Load the CSV files\n",
        "file_paths = ['/content/drive/MyDrive/Output/Aoutput_file0.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file1.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file2.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file3.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file4.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file5.csv']\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes into a single dataframe\n",
        "combined_df = pd.concat(dfs)\n",
        "\n",
        "# Check for missing or NaN values\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing or NaN values\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Preprocess the data\n",
        "X = combined_df[numeric_cols].values\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=128, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEU_3NR51g1L",
        "outputId": "89c933aa-8c3c-4eca-a85d-21f64748e40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "appeared                     0\n",
            "label                        0\n",
            "exports                      0\n",
            "has_debug                    0\n",
            "has_relocations              0\n",
            "has_resources                0\n",
            "has_signature                0\n",
            "has_tls                      0\n",
            "imports                      0\n",
            "size                         0\n",
            "symbols                      0\n",
            "vsize                        0\n",
            "coff_timestamp               0\n",
            "coff_machine                 8\n",
            "optional_subsystem         656\n",
            "optional_magic               0\n",
            "optional_sizeof_code         0\n",
            "optional_sizeof_headers      0\n",
            "numstrings                   0\n",
            "avlength                     0\n",
            "printables                   0\n",
            "entropy                      0\n",
            "paths                        0\n",
            "urls                         0\n",
            "registry                     0\n",
            "MZ                           0\n",
            "dtype: int64\n",
            "Epoch 1/50\n",
            "14/14 [==============================] - 6s 8ms/step - loss: 1.0347 - accuracy: 0.4580\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4917 - accuracy: 0.5307\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -0.6189 - accuracy: 0.7501\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -3.0609 - accuracy: 0.8043\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -8.2741 - accuracy: 0.7992\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -18.3561 - accuracy: 0.8009\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -37.1747 - accuracy: 0.7958\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -67.5917 - accuracy: 0.7908\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -123.3051 - accuracy: 0.8077\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -203.8188 - accuracy: 0.8043\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -319.1205 - accuracy: 0.8082\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -515.7593 - accuracy: 0.8150\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -795.3116 - accuracy: 0.8122\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -1148.3202 - accuracy: 0.8133\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -1633.7280 - accuracy: 0.8178\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -2267.9998 - accuracy: 0.8127\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -3074.4636 - accuracy: 0.8144\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -4186.4751 - accuracy: 0.8156\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -5476.7534 - accuracy: 0.8190\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -7313.5239 - accuracy: 0.8240\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -9105.1396 - accuracy: 0.8268\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -11557.2031 - accuracy: 0.8218\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -14744.5234 - accuracy: 0.8257\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -18650.9043 - accuracy: 0.8229\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -23196.3496 - accuracy: 0.8212\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -27943.5781 - accuracy: 0.8291\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -34612.5273 - accuracy: 0.8229\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -42520.9766 - accuracy: 0.8218\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -51431.4258 - accuracy: 0.8291\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -61094.5352 - accuracy: 0.8308\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -71992.5078 - accuracy: 0.8235\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -86963.4297 - accuracy: 0.8291\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -103525.9766 - accuracy: 0.8268\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -118412.1562 - accuracy: 0.8302\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -138252.4688 - accuracy: 0.8280\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -158917.0938 - accuracy: 0.8325\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -186897.4531 - accuracy: 0.8263\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -214224.5000 - accuracy: 0.8240\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -245952.9531 - accuracy: 0.8252\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -280838.5938 - accuracy: 0.8319\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -322740.7812 - accuracy: 0.8353\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -365636.4375 - accuracy: 0.8257\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -411551.7500 - accuracy: 0.8268\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -461615.3125 - accuracy: 0.8325\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -524412.6250 - accuracy: 0.8302\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -583650.3750 - accuracy: 0.8308\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -659754.4375 - accuracy: 0.8331\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -746247.4375 - accuracy: 0.8297\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -825148.9375 - accuracy: 0.8297\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -903638.6875 - accuracy: 0.8319\n",
            "24/24 [==============================] - 0s 6ms/step - loss: -811498.8750 - accuracy: 0.8384\n",
            "Test Accuracy: 0.8383705615997314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "# Load the CSV files\n",
        "file_paths = ['/content/drive/MyDrive/Output/Aoutput_file0.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file1.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file2.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file3.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file4.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file5.csv']\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes into a single dataframe\n",
        "combined_df = pd.concat(dfs)\n",
        "\n",
        "# Check for missing or NaN values\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing or NaN values\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Preprocess the data\n",
        "X = combined_df[numeric_cols].values\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=128, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEWguHlZgYvX",
        "outputId": "301064c9-e7e6-4923-f797-ab5249ff4f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "appeared                     0\n",
            "label                        0\n",
            "exports                      0\n",
            "has_debug                    0\n",
            "has_relocations              0\n",
            "has_resources                0\n",
            "has_signature                0\n",
            "has_tls                      0\n",
            "imports                      0\n",
            "size                         0\n",
            "symbols                      0\n",
            "vsize                        0\n",
            "coff_timestamp               0\n",
            "coff_machine                 8\n",
            "optional_subsystem         656\n",
            "optional_magic               0\n",
            "optional_sizeof_code         0\n",
            "optional_sizeof_headers      0\n",
            "numstrings                   0\n",
            "avlength                     0\n",
            "printables                   0\n",
            "entropy                      0\n",
            "paths                        0\n",
            "urls                         0\n",
            "registry                     0\n",
            "MZ                           0\n",
            "dtype: int64\n",
            "Epoch 1/50\n",
            "14/14 [==============================] - 3s 6ms/step - loss: 0.8957 - accuracy: 0.4879\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0718 - accuracy: 0.6881\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -1.4325 - accuracy: 0.8077\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -4.7942 - accuracy: 0.8116\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -10.8025 - accuracy: 0.7970\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -23.5695 - accuracy: 0.8071\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -45.4104 - accuracy: 0.7885\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -80.4152 - accuracy: 0.8094\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -143.7846 - accuracy: 0.7992\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -234.0483 - accuracy: 0.8043\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -373.9365 - accuracy: 0.8026\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -576.1560 - accuracy: 0.8099\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -886.6755 - accuracy: 0.8173\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -1292.6014 - accuracy: 0.8099\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -1804.2272 - accuracy: 0.8127\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -2543.0776 - accuracy: 0.8195\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -3544.9976 - accuracy: 0.8111\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -4736.8906 - accuracy: 0.8184\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -6297.8950 - accuracy: 0.8150\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -8187.6440 - accuracy: 0.8173\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -10496.7803 - accuracy: 0.8133\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -13385.3457 - accuracy: 0.8218\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -17114.8027 - accuracy: 0.8229\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -21211.0430 - accuracy: 0.8184\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -26313.2715 - accuracy: 0.8218\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -32232.6367 - accuracy: 0.8246\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -39952.1211 - accuracy: 0.8235\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -47507.7891 - accuracy: 0.8201\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -56762.8750 - accuracy: 0.8257\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -69401.1562 - accuracy: 0.8246\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -80279.6094 - accuracy: 0.8274\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -95054.2109 - accuracy: 0.8218\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -108924.5859 - accuracy: 0.8212\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -127315.1328 - accuracy: 0.8302\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -143719.6875 - accuracy: 0.8257\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -166611.7344 - accuracy: 0.8252\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -192211.9062 - accuracy: 0.8263\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -221732.6250 - accuracy: 0.8314\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -251399.5781 - accuracy: 0.8336\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -286162.6562 - accuracy: 0.8223\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -327137.2812 - accuracy: 0.8263\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -370817.4062 - accuracy: 0.8319\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -413205.7188 - accuracy: 0.8297\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -455661.1875 - accuracy: 0.8308\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -508353.9375 - accuracy: 0.8342\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -572118.8125 - accuracy: 0.8331\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -630519.1875 - accuracy: 0.8308\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -707493.8750 - accuracy: 0.8342\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -791591.0625 - accuracy: 0.8342\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -860110.2500 - accuracy: 0.8302\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -779859.0625 - accuracy: 0.8397\n",
            "Test Accuracy: 0.8396846055984497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load the CSV files\n",
        "file_paths = ['/content/drive/MyDrive/Output/Aoutput_file0.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file1.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file2.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file3.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file4.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file5.csv']\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes into a single dataframe\n",
        "combined_df = pd.concat(dfs)\n",
        "\n",
        "# Check for missing or NaN values\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing or NaN values\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Preprocess the data\n",
        "X = combined_df[numeric_cols].values\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Adjust learning rate\n",
        "learning_rate = 0.001\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=128, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wBSIKK8hckA",
        "outputId": "444c4a97-9df1-44fe-c003-885c20700173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "appeared                     0\n",
            "label                        0\n",
            "exports                      0\n",
            "has_debug                    0\n",
            "has_relocations              0\n",
            "has_resources                0\n",
            "has_signature                0\n",
            "has_tls                      0\n",
            "imports                      0\n",
            "size                         0\n",
            "symbols                      0\n",
            "vsize                        0\n",
            "coff_timestamp               0\n",
            "coff_machine                 8\n",
            "optional_subsystem         656\n",
            "optional_magic               0\n",
            "optional_sizeof_code         0\n",
            "optional_sizeof_headers      0\n",
            "numstrings                   0\n",
            "avlength                     0\n",
            "printables                   0\n",
            "entropy                      0\n",
            "paths                        0\n",
            "urls                         0\n",
            "registry                     0\n",
            "MZ                           0\n",
            "dtype: int64\n",
            "Epoch 1/50\n",
            "14/14 [==============================] - 1s 6ms/step - loss: 0.3170 - accuracy: 0.4698\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -1.4352 - accuracy: 0.6684\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -12.1519 - accuracy: 0.7783\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -72.9611 - accuracy: 0.7840\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -315.1893 - accuracy: 0.7953\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -1251.3849 - accuracy: 0.8139\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -3882.4370 - accuracy: 0.8116\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -10741.8320 - accuracy: 0.8178\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -25509.0820 - accuracy: 0.8212\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -53526.7539 - accuracy: 0.8206\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -106552.5312 - accuracy: 0.8263\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -198125.9844 - accuracy: 0.8285\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -340936.4375 - accuracy: 0.8268\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -566036.4375 - accuracy: 0.8206\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -901717.8125 - accuracy: 0.8285\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -1381147.2500 - accuracy: 0.8212\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -2067506.3750 - accuracy: 0.8308\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -2898823.7500 - accuracy: 0.8285\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -4121539.7500 - accuracy: 0.8223\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -5681235.5000 - accuracy: 0.8223\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -7609202.5000 - accuracy: 0.8252\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -10264265.0000 - accuracy: 0.8274\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -13408708.0000 - accuracy: 0.8274\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -17150538.0000 - accuracy: 0.8285\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -22270954.0000 - accuracy: 0.8206\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -28185918.0000 - accuracy: 0.8268\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -34823088.0000 - accuracy: 0.8240\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -43300628.0000 - accuracy: 0.8263\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -53146240.0000 - accuracy: 0.8257\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -63841596.0000 - accuracy: 0.8173\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -76849088.0000 - accuracy: 0.8235\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -93952160.0000 - accuracy: 0.8252\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -110068720.0000 - accuracy: 0.8240\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -132503704.0000 - accuracy: 0.8252\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -154918304.0000 - accuracy: 0.8235\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -177135792.0000 - accuracy: 0.8257\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -206284256.0000 - accuracy: 0.8212\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -241610048.0000 - accuracy: 0.8252\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -281133056.0000 - accuracy: 0.8235\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -320327200.0000 - accuracy: 0.8246\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -367990208.0000 - accuracy: 0.8246\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -418250784.0000 - accuracy: 0.8212\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -479956384.0000 - accuracy: 0.8212\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -531204096.0000 - accuracy: 0.8268\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -601637312.0000 - accuracy: 0.8223\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -677363776.0000 - accuracy: 0.8240\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -764398080.0000 - accuracy: 0.8246\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 5ms/step - loss: -859353728.0000 - accuracy: 0.8212\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 7ms/step - loss: -947041792.0000 - accuracy: 0.8280\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 6ms/step - loss: -1037820672.0000 - accuracy: 0.8240\n",
            "24/24 [==============================] - 0s 3ms/step - loss: -934613952.0000 - accuracy: 0.8305\n",
            "Test Accuracy: 0.8304861783981323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras import regularizers\n",
        "\n",
        "# Load the CSV files\n",
        "file_paths = ['/content/drive/MyDrive/Output/Aoutput_file0.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file1.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file2.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file3.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file4.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file5.csv']\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes into a single dataframe\n",
        "combined_df = pd.concat(dfs)\n",
        "\n",
        "# Check for missing or NaN values\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing or NaN values\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Preprocess the data\n",
        "X = combined_df[numeric_cols].values\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reshape X_train and X_test to have a sequential structure\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, activation='relu', input_shape=(1, X_train.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=128, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJteai3okMVL",
        "outputId": "17703fd0-880e-4e2a-d5dc-4fa3a934d2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "appeared                     0\n",
            "label                        0\n",
            "exports                      0\n",
            "has_debug                    0\n",
            "has_relocations              0\n",
            "has_resources                0\n",
            "has_signature                0\n",
            "has_tls                      0\n",
            "imports                      0\n",
            "size                         0\n",
            "symbols                      0\n",
            "vsize                        0\n",
            "coff_timestamp               0\n",
            "coff_machine                 8\n",
            "optional_subsystem         656\n",
            "optional_magic               0\n",
            "optional_sizeof_code         0\n",
            "optional_sizeof_headers      0\n",
            "numstrings                   0\n",
            "avlength                     0\n",
            "printables                   0\n",
            "entropy                      0\n",
            "paths                        0\n",
            "urls                         0\n",
            "registry                     0\n",
            "MZ                           0\n",
            "dtype: int64\n",
            "Epoch 1/50\n",
            "14/14 [==============================] - 5s 16ms/step - loss: 0.7418 - accuracy: 0.4653\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.7050 - accuracy: 0.4749\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.6604 - accuracy: 0.4670\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 0.6194 - accuracy: 0.4659\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.5616 - accuracy: 0.4647\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.5154 - accuracy: 0.4653\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.4449 - accuracy: 0.4676\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.3686 - accuracy: 0.4805\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.2878 - accuracy: 0.5082\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.1815 - accuracy: 0.5544\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0628 - accuracy: 0.6103\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: -0.0769 - accuracy: 0.6452\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -0.2146 - accuracy: 0.6870\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -0.3836 - accuracy: 0.7135\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -0.5654 - accuracy: 0.7507\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -0.8131 - accuracy: 0.7682\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -1.0679 - accuracy: 0.7795\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -1.3258 - accuracy: 0.7772\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -1.7611 - accuracy: 0.7970\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 8ms/step - loss: -2.1786 - accuracy: 0.7947\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -2.6633 - accuracy: 0.7981\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -3.1804 - accuracy: 0.7992\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: -3.9148 - accuracy: 0.8065\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -4.5760 - accuracy: 0.8167\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -5.6112 - accuracy: 0.8206\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -6.4626 - accuracy: 0.8218\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -7.2797 - accuracy: 0.8133\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -8.6215 - accuracy: 0.8150\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -9.9043 - accuracy: 0.8195\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -11.5751 - accuracy: 0.8144\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 14ms/step - loss: -13.8320 - accuracy: 0.8111\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -15.4976 - accuracy: 0.8184\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -17.4053 - accuracy: 0.8144\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -19.8160 - accuracy: 0.8133\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -22.0826 - accuracy: 0.8116\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 9ms/step - loss: -26.0518 - accuracy: 0.8105\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: -29.2880 - accuracy: 0.8184\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -31.7811 - accuracy: 0.8082\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -37.0448 - accuracy: 0.8116\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -40.2979 - accuracy: 0.8156\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -45.9813 - accuracy: 0.8167\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -52.0233 - accuracy: 0.8150\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -57.1182 - accuracy: 0.8139\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -62.1515 - accuracy: 0.8167\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -69.4019 - accuracy: 0.8206\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -74.2520 - accuracy: 0.8139\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: -80.8391 - accuracy: 0.8144\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -90.7481 - accuracy: 0.8161\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: -101.2534 - accuracy: 0.8223\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: -110.5338 - accuracy: 0.8161\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -97.2425 - accuracy: 0.8489\n",
            "Test Accuracy: 0.8488830327987671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "# Load the CSV files\n",
        "file_paths = ['/content/drive/MyDrive/Output/Aoutput_file0.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file1.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file2.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file3.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file4.csv',\n",
        "              '/content/drive/MyDrive/Output/Aoutput_file5.csv']\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all the dataframes into a single dataframe\n",
        "combined_df = pd.concat(dfs)\n",
        "\n",
        "# Check for missing or NaN values\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing or NaN values\n",
        "combined_df = combined_df.dropna()\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Preprocess the data\n",
        "X = combined_df[numeric_cols].values\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create the custom dense layer model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa33HJoUlr6e",
        "outputId": "d50da456-5d18-4cc0-a337-1f2f149decbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "appeared                     0\n",
            "label                        0\n",
            "exports                      0\n",
            "has_debug                    0\n",
            "has_relocations              0\n",
            "has_resources                0\n",
            "has_signature                0\n",
            "has_tls                      0\n",
            "imports                      0\n",
            "size                         0\n",
            "symbols                      0\n",
            "vsize                        0\n",
            "coff_timestamp               0\n",
            "coff_machine                 8\n",
            "optional_subsystem         656\n",
            "optional_magic               0\n",
            "optional_sizeof_code         0\n",
            "optional_sizeof_headers      0\n",
            "numstrings                   0\n",
            "avlength                     0\n",
            "printables                   0\n",
            "entropy                      0\n",
            "paths                        0\n",
            "urls                         0\n",
            "registry                     0\n",
            "MZ                           0\n",
            "dtype: int64\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 10s 8ms/step - loss: 0.7231 - accuracy: 0.4676\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.5660 - accuracy: 0.4721\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.3938 - accuracy: 0.4726\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1565 - accuracy: 0.4867\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -0.2614 - accuracy: 0.5505\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -0.9168 - accuracy: 0.6046\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -1.8788 - accuracy: 0.6791\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -3.0869 - accuracy: 0.7045\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -5.6805 - accuracy: 0.7310\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -9.3969 - accuracy: 0.7411\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -14.7136 - accuracy: 0.7625\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -24.4108 - accuracy: 0.7507\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -35.9309 - accuracy: 0.7654\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -52.2862 - accuracy: 0.7699\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -71.8576 - accuracy: 0.7761\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -102.7308 - accuracy: 0.7924\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -143.2122 - accuracy: 0.7766\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -191.1820 - accuracy: 0.7879\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -245.5053 - accuracy: 0.7947\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -318.0697 - accuracy: 0.7981\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 0s 12ms/step - loss: -411.3598 - accuracy: 0.7970\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 0s 12ms/step - loss: -511.5470 - accuracy: 0.8049\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -648.0418 - accuracy: 0.8060\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -804.7774 - accuracy: 0.8049\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -989.1050 - accuracy: 0.8032\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -1270.3579 - accuracy: 0.8088\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -1576.8589 - accuracy: 0.8082\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -1858.5730 - accuracy: 0.8133\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -2250.4976 - accuracy: 0.8026\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -2706.0244 - accuracy: 0.8144\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 0s 12ms/step - loss: -3199.8293 - accuracy: 0.8082\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -3903.8093 - accuracy: 0.8257\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -4628.0576 - accuracy: 0.8178\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -5377.4863 - accuracy: 0.8156\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -6322.1514 - accuracy: 0.8212\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -7087.7666 - accuracy: 0.8133\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -8223.2480 - accuracy: 0.8144\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -9486.9844 - accuracy: 0.8161\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -11045.4355 - accuracy: 0.8201\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -12588.4775 - accuracy: 0.8212\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -13857.4336 - accuracy: 0.8240\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -15632.5625 - accuracy: 0.8212\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -18481.1953 - accuracy: 0.8144\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -20248.5234 - accuracy: 0.8257\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -23088.3516 - accuracy: 0.8240\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -26057.9629 - accuracy: 0.8263\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -28982.2715 - accuracy: 0.8257\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -32921.8867 - accuracy: 0.8257\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -35443.6602 - accuracy: 0.8235\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -38941.5195 - accuracy: 0.8201\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -43465.3281 - accuracy: 0.8229\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -48637.0391 - accuracy: 0.8257\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -51769.7734 - accuracy: 0.8280\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -59298.7031 - accuracy: 0.8274\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -63024.1289 - accuracy: 0.8263\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -68400.3359 - accuracy: 0.8240\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -74604.0625 - accuracy: 0.8268\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -82913.7891 - accuracy: 0.8308\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -92914.4062 - accuracy: 0.8268\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -102176.5859 - accuracy: 0.8240\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -112439.5391 - accuracy: 0.8302\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -123392.4531 - accuracy: 0.8302\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -128920.4141 - accuracy: 0.8297\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -143440.0938 - accuracy: 0.8280\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -149697.5000 - accuracy: 0.8229\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -161460.9375 - accuracy: 0.8263\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -181441.5156 - accuracy: 0.8302\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -189834.8125 - accuracy: 0.8297\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -203314.4062 - accuracy: 0.8319\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -212962.7969 - accuracy: 0.8263\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -244717.4844 - accuracy: 0.8280\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -251981.1406 - accuracy: 0.8297\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -275303.2188 - accuracy: 0.8297\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 0s 7ms/step - loss: -303730.8125 - accuracy: 0.8308\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -319375.2188 - accuracy: 0.8268\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -354086.1250 - accuracy: 0.8268\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -364182.0938 - accuracy: 0.8257\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -397287.0000 - accuracy: 0.8252\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -421781.8125 - accuracy: 0.8257\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -468224.7500 - accuracy: 0.8325\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -478638.2812 - accuracy: 0.8297\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -515886.0312 - accuracy: 0.8302\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -547579.0000 - accuracy: 0.8280\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -570294.5000 - accuracy: 0.8297\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 0s 15ms/step - loss: -605501.2500 - accuracy: 0.8297\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 0s 12ms/step - loss: -663966.2500 - accuracy: 0.8308\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -708531.5000 - accuracy: 0.8302\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 0s 9ms/step - loss: -715941.6250 - accuracy: 0.8263\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 0s 8ms/step - loss: -774623.5000 - accuracy: 0.8291\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -833796.3750 - accuracy: 0.8319\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 0s 11ms/step - loss: -885554.8750 - accuracy: 0.8342\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 0s 14ms/step - loss: -924400.9375 - accuracy: 0.8297\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 0s 17ms/step - loss: -960124.5625 - accuracy: 0.8325\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 0s 15ms/step - loss: -993284.1250 - accuracy: 0.8274\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 0s 14ms/step - loss: -1034827.2500 - accuracy: 0.8268\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 0s 15ms/step - loss: -1137305.5000 - accuracy: 0.8285\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 0s 13ms/step - loss: -1184838.5000 - accuracy: 0.8331\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 0s 15ms/step - loss: -1247960.6250 - accuracy: 0.8297\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -1287304.0000 - accuracy: 0.8336\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 0s 10ms/step - loss: -1364820.8750 - accuracy: 0.8364\n",
            "24/24 [==============================] - 0s 4ms/step - loss: -1203689.7500 - accuracy: 0.8463\n",
            "Test Accuracy: 0.8462549448013306\n"
          ]
        }
      ]
    }
  ]
}